{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.layers import base\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "\n",
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"tensorflow\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model summary for tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "conv2d/kernel:0 (float32_ref 3x3x3x32) [864, bytes: 3456]\n",
      "conv2d/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "conv2d_1/kernel:0 (float32_ref 3x3x3x32) [864, bytes: 3456]\n",
      "conv2d_1/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "Total size of variables: 1792\n",
      "Total bytes of variables: 7168\n"
     ]
    }
   ],
   "source": [
    "# model summary for tensorflow\n",
    "x = np.zeros((1,4,4,3))\n",
    "x_tf = tf.convert_to_tensor(x, np.float32)\n",
    "z_tf = tf.layers.conv2d(x_tf, filters=32, kernel_size=(3,3))\n",
    "\n",
    "def model_summary():\n",
    "    model_vars = tf.trainable_variables()\n",
    "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
    "\n",
    "model_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspired from https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/1_Introduction/basic_operations.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 2 b: 3\n",
      "Addition with constants: 5\n",
      "Multiplication with constants: 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Basic constant operations\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "\n",
    "# Launch the default graph.\n",
    "with tf.Session() as sess:\n",
    "    print (\"a: %i\" % sess.run(a), \"b: %i\" % sess.run(b))\n",
    "    print (\"Addition with constants: %i\" % sess.run(a+b))\n",
    "    print (\"Multiplication with constants: %i\" % sess.run(a*b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/vahidk/EffectiveTensorflow\n",
    "### Numpy version of below\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.normal(size=[10, 10])\n",
    "\n",
    "y = np.random.normal(size=[10, 10])\n",
    "\n",
    "z = np.dot(x, y)\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.34614965 -7.3237243   0.9355222   0.7125121   4.2473693  -3.2875426\n",
      "   2.6320853   0.60445595 -1.8011832  -1.4778548 ]\n",
      " [-8.892176    2.2700188   0.84772235  0.44044608  1.9714642   0.20464115\n",
      "  -1.4039882   1.1470723   0.18000975  0.34565252]\n",
      " [ 1.6059456  -4.0441747   2.264099   -2.958685    0.69468784  0.38571823\n",
      "   5.6918163   1.4194463   1.7618747  -4.2023764 ]\n",
      " [-1.2256149   2.5026336  -0.5831862   1.1683558   0.60250676  0.34096497\n",
      "  -2.0679157   1.0514991  -2.714299    0.22064567]\n",
      " [-5.664401   -4.645505   -1.6683139   2.680467    0.61823153  1.5860802\n",
      "  -4.2265906   2.8202052  -0.52584696 -3.7972417 ]\n",
      " [-5.008916   -3.762639   -0.66483474  3.618266    3.3554142  -5.250147\n",
      "  -0.41241854  0.91815126 -4.481329   -2.478259  ]\n",
      " [ 4.498228    2.0109146   0.91723716  1.3889239   0.18919843 -7.4312754\n",
      "   3.381404    0.9045108  -6.4896193   2.0509806 ]\n",
      " [ 1.5027142   1.0791501  -0.530404   -0.5126542   1.0960672   2.2281358\n",
      "   0.20963605  0.53031576  1.7040433   2.4191246 ]\n",
      " [-5.358087   -1.2983735  -2.6722093   3.1247656   1.0554792   1.8205503\n",
      "  -3.9763834   0.13570663  1.3080064  -0.42651823]\n",
      " [-8.480544   -5.7079782   3.4556909   2.3919075   3.5993748  -8.836415\n",
      "  -0.9253386   2.1694067  -7.6993933  -1.2274268 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.random_normal([10, 10])\n",
    "y = tf.random_normal([10, 10])\n",
    "z = tf.matmul(x, y)\n",
    "\n",
    "sess = tf.Session()\n",
    "z_val = sess.run(z)\n",
    "\n",
    "print(z_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note the below doesn't explain the basics. hence skip the below for a while\n",
    "To understand how powerful symbolic computation can be let's have a look at another example. Assume that we have samples from a curve (say f(x) = 5x^2 + 3) and we want to estimate f(x) based on these samples. We define a parametric function g(x, w) = w0 x^2 + w1 x + w2, which is a function of the input x and latent parameters w, our goal is then to find the latent parameters such that g(x, w) ≈ f(x). This can be done by minimizing the following loss function: L(w) = ∑ (f(x) - g(x, w))^2. Although there's a closed form solution for this simple problem, we opt to use a more general approach that can be applied to any arbitrary differentiable function, and that is using stochastic gradient descent. We simply compute the average gradient of L(w) with respect to w over a set of sample points and move in the opposite direction.\n",
    "\n",
    "Here's how it can be done in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 4.9953189e+00],\n",
      "       [-5.3842890e-05],\n",
      "       [ 3.2785773e+00]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders are used to feed values from python to TensorFlow ops. We define\n",
    "# two placeholders, one for input feature x, and one for output y.\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Assuming we know that the desired function is a polynomial of 2nd degree, we\n",
    "# allocate a vector of size 3 to hold the coefficients. The variable will be\n",
    "# automatically initialized with random noise.\n",
    "w = tf.get_variable(\"w\", shape=[3, 1])\n",
    "\n",
    "# We define yhat to be our estimate of y.\n",
    "f = tf.stack([tf.square(x), x, tf.ones_like(x)], 1)\n",
    "yhat = tf.squeeze(tf.matmul(f, w), 1)\n",
    "\n",
    "# The loss is defined to be the l2 distance between our estimate of y and its\n",
    "# true value. We also added a shrinkage term, to ensure the resulting weights\n",
    "# would be small.\n",
    "loss = tf.nn.l2_loss(yhat - y) + 0.1 * tf.nn.l2_loss(w)\n",
    "\n",
    "# We use the Adam optimizer with learning rate set to 0.1 to minimize the loss.\n",
    "train_op = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "\n",
    "def generate_data():\n",
    "    x_val = np.random.uniform(-10.0, 10.0, size=100)\n",
    "    y_val = 5 * np.square(x_val) + 3\n",
    "    return x_val, y_val\n",
    "\n",
    "sess = tf.Session()\n",
    "# Since we are using variables we first need to initialize them.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for _ in range(1000):\n",
    "    x_val, y_val = generate_data()\n",
    "    _, loss_val = sess.run([train_op, loss], {x: x_val, y: y_val})\n",
    "    #print(loss_val)\n",
    "print(sess.run([w]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "Normal Equation (θ = XT · X)–1 · XT · y for house market prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7185181e+01]\n",
      " [ 4.3633747e-01]\n",
      " [ 9.3952334e-03]\n",
      " [-1.0711310e-01]\n",
      " [ 6.4479220e-01]\n",
      " [-4.0338000e-06]\n",
      " [-3.7813708e-03]\n",
      " [-4.2348403e-01]\n",
      " [-4.3721911e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "tf.reset_default_graph()\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "with tf.Session() as sess: \n",
    "    theta_value = theta.eval()\n",
    "    print(theta_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Pure numpy version\n",
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00  6.60969987e-17  5.50808322e-18  6.60969987e-17\n",
      " -1.06030602e-16 -1.10161664e-17  3.44255201e-18 -1.07958431e-15\n",
      " -8.52651283e-15]\n",
      "[ 0.38915536  0.36424355  0.5116157  ... -0.06612179 -0.06360587\n",
      "  0.01359031]\n",
      "0.11111111111111005\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "\n",
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.71450067\n",
      "Epoch 200 MSE = 0.5667049\n",
      "Epoch 300 MSE = 0.5555719\n",
      "Epoch 400 MSE = 0.5488112\n",
      "Epoch 500 MSE = 0.5436362\n",
      "Epoch 600 MSE = 0.5396294\n",
      "Epoch 700 MSE = 0.53650916\n",
      "Epoch 800 MSE = 0.5340678\n",
      "Epoch 900 MSE = 0.5321474\n",
      "[[ 2.0685523 ]\n",
      " [ 0.8874027 ]\n",
      " [ 0.14401656]\n",
      " [-0.34770885]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.66145283]\n",
      " [-0.6375278 ]]\n"
     ]
    }
   ],
   "source": [
    "#Gradient Descent\n",
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.7145006\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.5555719\n",
      "Epoch 400 MSE = 0.5488112\n",
      "Epoch 500 MSE = 0.5436362\n",
      "Epoch 600 MSE = 0.5396294\n",
      "Epoch 700 MSE = 0.5365092\n",
      "Epoch 800 MSE = 0.5340678\n",
      "Epoch 900 MSE = 0.5321474\n",
      "Best theta:\n",
      "[[ 2.0685525 ]\n",
      " [ 0.8874027 ]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.6614528 ]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "# Using autodiff\n",
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Example\n",
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/linear_regression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost= 0.126894742 W= 0.3745814 b= -0.097686656\n",
      "Epoch: 0100 cost= 0.121124744 W= 0.36714777 b= -0.044209745\n",
      "Epoch: 0150 cost= 0.116021343 W= 0.36015627 b= 0.00608666\n",
      "Epoch: 0200 cost= 0.111507520 W= 0.35358056 b= 0.05339157\n",
      "Epoch: 0250 cost= 0.107515201 W= 0.3473959 b= 0.09788313\n",
      "Epoch: 0300 cost= 0.103984140 W= 0.3415792 b= 0.1397285\n",
      "Epoch: 0350 cost= 0.100861095 W= 0.33610845 b= 0.1790852\n",
      "Epoch: 0400 cost= 0.098098941 W= 0.33096302 b= 0.21610112\n",
      "Epoch: 0450 cost= 0.095655993 W= 0.3261235 b= 0.25091574\n",
      "Epoch: 0500 cost= 0.093495443 W= 0.3215719 b= 0.28365952\n",
      "Epoch: 0550 cost= 0.091584556 W= 0.31729096 b= 0.31445622\n",
      "Epoch: 0600 cost= 0.089894608 W= 0.31326464 b= 0.343421\n",
      "Epoch: 0650 cost= 0.088400029 W= 0.3094778 b= 0.3706633\n",
      "Epoch: 0700 cost= 0.087078258 W= 0.3059162 b= 0.39628544\n",
      "Epoch: 0750 cost= 0.085909322 W= 0.30256656 b= 0.42038321\n",
      "Epoch: 0800 cost= 0.084875606 W= 0.29941604 b= 0.44304764\n",
      "Epoch: 0850 cost= 0.083961442 W= 0.29645273 b= 0.46436492\n",
      "Epoch: 0900 cost= 0.083152987 W= 0.29366568 b= 0.48441482\n",
      "Epoch: 0950 cost= 0.082438134 W= 0.29104447 b= 0.50327134\n",
      "Epoch: 1000 cost= 0.081805959 W= 0.28857931 b= 0.5210061\n",
      "Optimization Finished!\n",
      "Training cost= 0.08180596 W= 0.28857931 b= 0.5210061 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = np.random\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1000\n",
    "display_step = 50\n",
    "\n",
    "# Training Data\n",
    "train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = train_X.shape[0]\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "\n",
    "# Set model weights and bias random\n",
    "W = tf.Variable(rng.randn(), name=\"weight\")\n",
    "b = tf.Variable(rng.randn(), name=\"bias\")\n",
    "\n",
    "# Construct a linear model. y = XW + b\n",
    "pred = tf.add(tf.multiply(X, W), b)\n",
    "\n",
    "# Mean squared error\n",
    "cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n",
    "\n",
    "# Gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (x, y) in zip(train_X, train_Y):\n",
    "            sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n",
    "                \"W = \", sess.run(W), \"b = \", sess.run(b))\n",
    "\n",
    "    print (\"Optimization Finished!\")\n",
    "    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n",
    "    print (\"Training cost=\", training_cost, \"W = \", sess.run(W), \"b = \", sess.run(b), '\\n')\n",
    "\n",
    "    #Graphic display\n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Example\n",
    "\n",
    "# Import MINST data\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.184351654\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.03730078  0.079611   -0.01896858 -0.02118796  0.01909295  0.0290922\n",
      " -0.0093307   0.03109496 -0.06625962 -0.00584353]\n",
      "Epoch: 0002 cost= 0.665294986\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.05097382  0.11064371 -0.02714274 -0.03299111  0.03137657  0.06248994\n",
      " -0.01303456  0.0533782  -0.11813473 -0.01561154]\n",
      "Epoch: 0003 cost= 0.552845062\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.06005226  0.13130666 -0.03169165 -0.04252985  0.03687295  0.09206253\n",
      " -0.01484207  0.07162529 -0.161586   -0.02116575]\n",
      "Epoch: 0004 cost= 0.498638791\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.06812466  0.14778477 -0.03708484 -0.04939237  0.04321228  0.12082824\n",
      " -0.01635786  0.087356   -0.19968395 -0.02853803]\n",
      "Epoch: 0005 cost= 0.465521762\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.07547318  0.1604302  -0.03652826 -0.05572912  0.04746671  0.14966229\n",
      " -0.01817798  0.09991929 -0.2383257  -0.03324459]\n",
      "Epoch: 0006 cost= 0.442640048\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.08153969  0.17013672 -0.03679978 -0.06331212  0.04944078  0.17483146\n",
      " -0.01861076  0.1140615  -0.2693314  -0.03887695]\n",
      "Epoch: 0007 cost= 0.425481379\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.08848141  0.18146156 -0.03798386 -0.06761739  0.05332766  0.19949351\n",
      " -0.01861517  0.12466203 -0.30236405 -0.04388312]\n",
      "Epoch: 0008 cost= 0.412171161\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.09313428  0.18949196 -0.03838684 -0.07316037  0.05374273  0.22340766\n",
      " -0.02087186  0.13762471 -0.33184364 -0.04686971]\n",
      "Epoch: 0009 cost= 0.401392119\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.0990689   0.1960882  -0.03592963 -0.07843751  0.05531807  0.24680783\n",
      " -0.02088237  0.14924437 -0.3616927  -0.05144653]\n",
      "Epoch: 0010 cost= 0.392403340\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.10445043  0.20348659 -0.03892315 -0.0832253   0.05772278  0.27106145\n",
      " -0.02252211  0.1597161  -0.38710326 -0.05576197]\n",
      "Epoch: 0011 cost= 0.384773500\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.10876292  0.21040373 -0.0357703  -0.08684084  0.05734215  0.29051816\n",
      " -0.02293032  0.16954266 -0.4133052  -0.06019624]\n",
      "Epoch: 0012 cost= 0.378178392\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.11365984  0.21648863 -0.03518485 -0.0926086   0.0592064   0.31241697\n",
      " -0.02448673  0.17998059 -0.44008526 -0.06206581]\n",
      "Epoch: 0013 cost= 0.372386288\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.11877178  0.22191867 -0.03602133 -0.09552856  0.06074506  0.3333259\n",
      " -0.02509693  0.18969345 -0.463182   -0.06708133]\n",
      "Epoch: 0014 cost= 0.367272738\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.12416838  0.2257404  -0.03433709 -0.09767388  0.06083799  0.3527775\n",
      " -0.02654182  0.199941   -0.48622563 -0.07034939]\n",
      "Epoch: 0015 cost= 0.362750185\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.12856363  0.23138134 -0.03092325 -0.10336305  0.06109109  0.37260556\n",
      " -0.02755004  0.20756975 -0.50907403 -0.07317296]\n",
      "Epoch: 0016 cost= 0.358613667\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.1326521   0.23466517 -0.0313612  -0.1057642   0.06152226  0.3914546\n",
      " -0.0270358   0.21647635 -0.53087926 -0.07642572]\n",
      "Epoch: 0017 cost= 0.354890988\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.13678822  0.2400118  -0.02834421 -0.10973241  0.06161172  0.41064554\n",
      " -0.02878498  0.22524315 -0.5525351  -0.08132744]\n",
      "Epoch: 0018 cost= 0.351455844\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.14169511  0.24368851 -0.02746959 -0.11422423  0.06051907  0.4298534\n",
      " -0.02932757  0.2341022  -0.57317317 -0.08227421]\n",
      "Epoch: 0019 cost= 0.348301004\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.14530456  0.24754585 -0.0264463  -0.11690253  0.06098108  0.4471063\n",
      " -0.02999718  0.24277438 -0.59326774 -0.08648942]\n",
      "Epoch: 0020 cost= 0.345450270\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.14967677  0.25064653 -0.02487657 -0.11899381  0.06016582  0.4659972\n",
      " -0.03192828  0.25152162 -0.61397874 -0.08887698]\n",
      "Epoch: 0021 cost= 0.342746153\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.15434207  0.25267965 -0.02194185 -0.12138518  0.06083791  0.48278758\n",
      " -0.0336969   0.25966427 -0.6317692  -0.09283436]\n",
      "Epoch: 0022 cost= 0.340233801\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.15849629  0.2573678  -0.02097109 -0.12418069  0.06164705  0.50061244\n",
      " -0.03446113  0.26553428 -0.65165555 -0.09539565]\n",
      "Epoch: 0023 cost= 0.337906543\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.161714    0.2599396  -0.01815308 -0.1288951   0.05983865  0.51536673\n",
      " -0.03468929  0.27460033 -0.66929084 -0.09700102]\n",
      "Epoch: 0024 cost= 0.335751848\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.16501151  0.2627594  -0.01875818 -0.1301892   0.06000581  0.532264\n",
      " -0.03506881  0.28079903 -0.68712956 -0.09966965]\n",
      "Epoch: 0025 cost= 0.333729791\n",
      "W =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] b =  [-0.17010799  0.26588017 -0.01508724 -0.13369185  0.06029621  0.5489422\n",
      " -0.03660642  0.28865454 -0.70499253 -0.10328635]\n",
      "Optimization Finished!\n",
      "Accuracy: 0.889\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "            print (\"W = \", sess.run(W), \"b = \", sess.run(b))\n",
    "\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print (\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple neural network\n",
    "A 2-Hidden Layers Fully Connected Neural Network (a.k.a Multilayer Perceptron)\n",
    "implementation with TensorFlow. This example is using the MNIST database\n",
    "of handwritten digits (http://yann.lecun.com/exdb/mnist/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_nn (layer1_n,layer2_n) : \n",
    "    print(\"Starting simple Neural network \" + str(layer1_n) +\" : \" + str(layer2_n) )\n",
    "    reset_graph()\n",
    "    # Parameters\n",
    "    learning_rate = 0.1\n",
    "    num_steps = 500\n",
    "    batch_size = 128\n",
    "    display_step = 100\n",
    "\n",
    "    # Network Parameters\n",
    "    n_hidden_1 = layer1_n # 1st layer number of neurons\n",
    "    n_hidden_2 = layer2_n # 2nd layer number of neurons\n",
    "    num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "    num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "    }\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    def neural_net(x):\n",
    "        # Hidden fully connected layer with layer 1 neurons\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        # Hidden fully connected layer with layer 2 neurons\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        # Output fully connected layer with a neuron for each class\n",
    "        out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "        return out_layer\n",
    "\n",
    "    # Construct model\n",
    "    logits = neural_net(X)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # display all the parameters\n",
    "    vars = 0\n",
    "    for v in tf.all_variables():\n",
    "        vars += np.prod(v.get_shape().as_list())\n",
    "    print(vars)\n",
    "    \n",
    "    # Start training\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "\n",
    "        for step in range(1, num_steps+1):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "            if step % display_step == 0 or step == 1:\n",
    "                # Calculate batch loss and accuracy\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                     Y: batch_y})\n",
    "                print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        # Calculate accuracy for MNIST test images\n",
    "        print(\"Testing Accuracy:\", \\\n",
    "            sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "                                          Y: mnist.test.labels}))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simple Neural network 2048 : 2048\n",
      "17473568.0\n",
      "Step 1, Minibatch Loss= 474724.2500, Training Accuracy= 0.352\n",
      "Step 100, Minibatch Loss= 12443.1035, Training Accuracy= 0.867\n",
      "Step 200, Minibatch Loss= 6874.4072, Training Accuracy= 0.852\n",
      "Step 300, Minibatch Loss= 1211.8154, Training Accuracy= 0.914\n",
      "Step 400, Minibatch Loss= 1998.8522, Training Accuracy= 0.875\n",
      "Step 500, Minibatch Loss= 1319.4133, Training Accuracy= 0.852\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8519\n",
      "Starting simple Neural network 1024 : 1024\n",
      "5591072.0\n",
      "Step 1, Minibatch Loss= 103726.3438, Training Accuracy= 0.484\n",
      "Step 100, Minibatch Loss= 6831.8525, Training Accuracy= 0.852\n",
      "Step 200, Minibatch Loss= 2858.8306, Training Accuracy= 0.836\n",
      "Step 300, Minibatch Loss= 1054.2766, Training Accuracy= 0.914\n",
      "Step 400, Minibatch Loss= 358.0754, Training Accuracy= 0.883\n",
      "Step 500, Minibatch Loss= 553.5654, Training Accuracy= 0.867\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8651\n",
      "Starting simple Neural network 512 : 512\n",
      "2009120.0\n",
      "Step 1, Minibatch Loss= 35562.5625, Training Accuracy= 0.289\n",
      "Step 100, Minibatch Loss= 704.7876, Training Accuracy= 0.930\n",
      "Step 200, Minibatch Loss= 479.4126, Training Accuracy= 0.875\n",
      "Step 300, Minibatch Loss= 245.0352, Training Accuracy= 0.844\n",
      "Step 400, Minibatch Loss= 143.5513, Training Accuracy= 0.883\n",
      "Step 500, Minibatch Loss= 82.5241, Training Accuracy= 0.891\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8675\n",
      "Starting simple Neural network 256 : 256\n",
      "807968.0\n",
      "Step 1, Minibatch Loss= 15379.7881, Training Accuracy= 0.219\n",
      "Step 100, Minibatch Loss= 328.2344, Training Accuracy= 0.844\n",
      "Step 200, Minibatch Loss= 158.4039, Training Accuracy= 0.875\n",
      "Step 300, Minibatch Loss= 148.0388, Training Accuracy= 0.828\n",
      "Step 400, Minibatch Loss= 40.8494, Training Accuracy= 0.891\n",
      "Step 500, Minibatch Loss= 19.9304, Training Accuracy= 0.922\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8473\n",
      "Starting simple Neural network 128 : 128\n",
      "354848.0\n",
      "Step 1, Minibatch Loss= 3644.5850, Training Accuracy= 0.359\n",
      "Step 100, Minibatch Loss= 69.0138, Training Accuracy= 0.883\n",
      "Step 200, Minibatch Loss= 46.2350, Training Accuracy= 0.906\n",
      "Step 300, Minibatch Loss= 13.0044, Training Accuracy= 0.898\n",
      "Step 400, Minibatch Loss= 17.5829, Training Accuracy= 0.875\n",
      "Step 500, Minibatch Loss= 14.1382, Training Accuracy= 0.836\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8328\n",
      "Starting simple Neural network 24 : 24\n",
      "59072.0\n",
      "Step 1, Minibatch Loss= 438.1121, Training Accuracy= 0.164\n",
      "Step 100, Minibatch Loss= 5.8225, Training Accuracy= 0.859\n",
      "Step 200, Minibatch Loss= 2.8557, Training Accuracy= 0.906\n",
      "Step 300, Minibatch Loss= 2.7573, Training Accuracy= 0.828\n",
      "Step 400, Minibatch Loss= 1.4487, Training Accuracy= 0.828\n",
      "Step 500, Minibatch Loss= 0.5442, Training Accuracy= 0.875\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8594\n"
     ]
    }
   ],
   "source": [
    "simple_nn(2048,2048)\n",
    "simple_nn(1024,1024)\n",
    "simple_nn(512,512)\n",
    "simple_nn(256,256)\n",
    "simple_nn(128,128)\n",
    "simple_nn(24,24)\n",
    "# 256 : 256 > 807968.0\n",
    "# 512 : 512 > 2009120.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simple Neural network 512 : 128\n",
      "1406624.0\n",
      "Step 1, Minibatch Loss= 15699.5000, Training Accuracy= 0.258\n",
      "Step 100, Minibatch Loss= 399.3960, Training Accuracy= 0.844\n",
      "Step 200, Minibatch Loss= 127.8773, Training Accuracy= 0.906\n",
      "Step 300, Minibatch Loss= 94.4084, Training Accuracy= 0.828\n",
      "Step 400, Minibatch Loss= 23.0377, Training Accuracy= 0.930\n",
      "Step 500, Minibatch Loss= 21.7889, Training Accuracy= 0.906\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.842\n",
      "Starting simple Neural network 12 : 12\n",
      "29120.0\n",
      "Step 1, Minibatch Loss= 173.4673, Training Accuracy= 0.117\n",
      "Step 100, Minibatch Loss= 1.9999, Training Accuracy= 0.773\n",
      "Step 200, Minibatch Loss= 0.7199, Training Accuracy= 0.836\n",
      "Step 300, Minibatch Loss= 0.4309, Training Accuracy= 0.875\n",
      "Step 400, Minibatch Loss= 0.5109, Training Accuracy= 0.867\n",
      "Step 500, Minibatch Loss= 0.7298, Training Accuracy= 0.852\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.861\n"
     ]
    }
   ],
   "source": [
    "simple_nn(512,128)\n",
    "simple_nn(12,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network (tf.layers/estimator api) \n",
    "A 2-Hidden Layers Fully Connected Neural Network (a.k.a Multilayer Perceptron)\n",
    "implementation with TensorFlow. This example is using the MNIST database\n",
    "of handwritten digits (http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "This example is using TensorFlow layers, see 'neural_network_raw' example for\n",
    "a raw implementation with variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/5h/qfyzgyfn5rg6s6vt938bsm0w0000gn/T/tmpol911jvr\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/5h/qfyzgyfn5rg6s6vt938bsm0w0000gn/T/tmpol911jvr', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c40d18080>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Failed to convert object of type <class 'dict'> to Tensor. Contents: {'images': <tf.Tensor 'random_shuffle_queue_DequeueMany:1' shape=(128, 784) dtype=float32>}. Consider casting elements to a supported type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     60\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 61\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got {'images': <tf.Tensor 'random_shuffle_queue_DequeueMany:1' shape=(128, 784) dtype=float32>}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-b5f3f87da6ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimple_nn_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-93-7f404fb1ccc2>\u001b[0m in \u001b[0;36msimple_nn_layer\u001b[0;34m(layer1_n, layer2_n)\u001b[0m\n\u001b[1;32m     69\u001b[0m         batch_size=batch_size, num_epochs=None, shuffle=True)\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# Train the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Evaluate the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1205\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1235\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1237\u001b[0;31m           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m   1238\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-7f404fb1ccc2>\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Build the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-ec5c345fde51>\u001b[0m in \u001b[0;36mneural_net\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mneural_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Hidden fully connected layer with 256 neurons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mlayer_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Hidden fully connected layer with 256 neurons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mlayer_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1999\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2001\u001b[0;31m       \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2002\u001b[0m       \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    227\u001b[0m                                          as_ref=False):\n\u001b[1;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    206\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 208\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    529\u001b[0m       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\u001b[1;32m    530\u001b[0m                       \u001b[0;34m\"Contents: %s. Consider casting elements to a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                       \"supported type.\" % (type(values), values))\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'images': <tf.Tensor 'random_shuffle_queue_DequeueMany:1' shape=(128, 784) dtype=float32>}. Consider casting elements to a supported type."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-3e4d05894feb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
